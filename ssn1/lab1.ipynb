{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAB 1 SSN MICHAŁ SIENKIEWICZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Definicja pierwszej warstwy konwolucyjnej: wejście 3 kanały (RGB), wyjście 32 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        # Definicja drugiej warstwy konwolucyjnej: wejście 32 kanały, wyjście 64 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # Definicja trzeciej warstwy konwolucyjnej: wejście 64 kanały, wyjście 128 kanały, rozmiar filtra 3x3, padding=1\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        # Warstwa dropout1 z prawdopodobieństwem wyzerowania 25% neuronów\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # Warstwa dropout2 z prawdopodobieństwem wyzerowania 50% neuronów\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Warstwa w pełni połączona (fully connected), wejście 128 * 4 * 4 neurony (wynik rozmiaru tensora po ostatniej konwolucji), wyjście 512 neurony\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        # Warstwa w pełni połączona (fully connected), wejście 512 neurony, wyjście 10 neurony (liczba klas)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Przekształcenie wejścia przez pierwszą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Przekształcenie przez drugą warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Przekształcenie przez trzecią warstwę konwolucyjną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        # Redukcja wymiarowości przez operację max-pooling z rozmiarem okna 2x2\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        # Wykorzystanie warstwy dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Spłaszczenie tensora do postaci wektora przed podaniem na warstwę w pełni połączoną\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Przekształcenie przez warstwę w pełni połączoną, zastosowanie funkcji aktywacji ReLU\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        # Wykorzystanie warstwy dropout2\n",
    "        x = self.dropout2(x)\n",
    "        # Przekształcenie przez warstwę w pełni połączoną (wyjściową)\n",
    "        x = self.fc2(x)\n",
    "        # Zastosowanie funkcji log_softmax dla uzyskania prawdopodobieństw przynależności do klas\n",
    "        output = nn.functional.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 2.271\n",
      "[1, 200] loss: 2.090\n",
      "[1, 300] loss: 1.950\n",
      "[1, 400] loss: 1.862\n",
      "[1, 500] loss: 1.759\n",
      "[1, 600] loss: 1.720\n",
      "[1, 700] loss: 1.641\n",
      "[2, 100] loss: 1.587\n",
      "[2, 200] loss: 1.547\n",
      "[2, 300] loss: 1.531\n",
      "[2, 400] loss: 1.459\n",
      "[2, 500] loss: 1.441\n",
      "[2, 600] loss: 1.442\n",
      "[2, 700] loss: 1.413\n",
      "[3, 100] loss: 1.324\n",
      "[3, 200] loss: 1.358\n",
      "[3, 300] loss: 1.317\n",
      "[3, 400] loss: 1.276\n",
      "[3, 500] loss: 1.244\n",
      "[3, 600] loss: 1.214\n",
      "[3, 700] loss: 1.223\n",
      "[4, 100] loss: 1.180\n",
      "[4, 200] loss: 1.171\n",
      "[4, 300] loss: 1.121\n",
      "[4, 400] loss: 1.119\n",
      "[4, 500] loss: 1.108\n",
      "[4, 600] loss: 1.116\n",
      "[4, 700] loss: 1.104\n",
      "[5, 100] loss: 1.058\n",
      "[5, 200] loss: 1.046\n",
      "[5, 300] loss: 1.050\n",
      "[5, 400] loss: 1.043\n",
      "[5, 500] loss: 1.017\n",
      "[5, 600] loss: 1.009\n",
      "[5, 700] loss: 0.998\n",
      "[6, 100] loss: 0.975\n",
      "[6, 200] loss: 0.954\n",
      "[6, 300] loss: 0.963\n",
      "[6, 400] loss: 0.974\n",
      "[6, 500] loss: 0.955\n",
      "[6, 600] loss: 0.949\n",
      "[6, 700] loss: 0.950\n",
      "[7, 100] loss: 0.927\n",
      "[7, 200] loss: 0.924\n",
      "[7, 300] loss: 0.923\n",
      "[7, 400] loss: 0.916\n",
      "[7, 500] loss: 0.916\n",
      "[7, 600] loss: 0.896\n",
      "[7, 700] loss: 0.865\n",
      "[8, 100] loss: 0.875\n",
      "[8, 200] loss: 0.861\n",
      "[8, 300] loss: 0.909\n",
      "[8, 400] loss: 0.841\n",
      "[8, 500] loss: 0.873\n",
      "[8, 600] loss: 0.844\n",
      "[8, 700] loss: 0.870\n",
      "[9, 100] loss: 0.834\n",
      "[9, 200] loss: 0.835\n",
      "[9, 300] loss: 0.863\n",
      "[9, 400] loss: 0.839\n",
      "[9, 500] loss: 0.836\n",
      "[9, 600] loss: 0.858\n",
      "[9, 700] loss: 0.775\n",
      "[10, 100] loss: 0.817\n",
      "[10, 200] loss: 0.816\n",
      "[10, 300] loss: 0.817\n",
      "[10, 400] loss: 0.821\n",
      "[10, 500] loss: 0.798\n",
      "[10, 600] loss: 0.814\n",
      "[10, 700] loss: 0.809\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 74.56%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZAD.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotowanie transformacji dla zestawu danych MNIST (liczby ręcznie pisane)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(((0.5,)),  (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pobranie zestawu danych MNIST\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model sieci neuronowej dla MNIST\n",
    "class NetMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = nn.functional.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetMNIST()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.440\n",
      "[1, 200] loss: 0.428\n",
      "[1, 300] loss: 0.265\n",
      "[1, 400] loss: 0.198\n",
      "[1, 500] loss: 0.168\n",
      "[1, 600] loss: 0.161\n",
      "[1, 700] loss: 0.140\n",
      "[1, 800] loss: 0.138\n",
      "[1, 900] loss: 0.126\n",
      "[2, 100] loss: 0.105\n",
      "[2, 200] loss: 0.102\n",
      "[2, 300] loss: 0.114\n",
      "[2, 400] loss: 0.110\n",
      "[2, 500] loss: 0.097\n",
      "[2, 600] loss: 0.105\n",
      "[2, 700] loss: 0.087\n",
      "[2, 800] loss: 0.093\n",
      "[2, 900] loss: 0.090\n",
      "[3, 100] loss: 0.079\n",
      "[3, 200] loss: 0.088\n",
      "[3, 300] loss: 0.084\n",
      "[3, 400] loss: 0.077\n",
      "[3, 500] loss: 0.080\n",
      "[3, 600] loss: 0.072\n",
      "[3, 700] loss: 0.073\n",
      "[3, 800] loss: 0.075\n",
      "[3, 900] loss: 0.088\n",
      "[4, 100] loss: 0.084\n",
      "[4, 200] loss: 0.070\n",
      "[4, 300] loss: 0.072\n",
      "[4, 400] loss: 0.072\n",
      "[4, 500] loss: 0.064\n",
      "[4, 600] loss: 0.067\n",
      "[4, 700] loss: 0.064\n",
      "[4, 800] loss: 0.064\n",
      "[4, 900] loss: 0.062\n",
      "[5, 100] loss: 0.054\n",
      "[5, 200] loss: 0.054\n",
      "[5, 300] loss: 0.060\n",
      "[5, 400] loss: 0.067\n",
      "[5, 500] loss: 0.066\n",
      "[5, 600] loss: 0.059\n",
      "[5, 700] loss: 0.060\n",
      "[5, 800] loss: 0.060\n",
      "[5, 900] loss: 0.058\n",
      "[6, 100] loss: 0.053\n",
      "[6, 200] loss: 0.058\n",
      "[6, 300] loss: 0.053\n",
      "[6, 400] loss: 0.047\n",
      "[6, 500] loss: 0.054\n",
      "[6, 600] loss: 0.051\n",
      "[6, 700] loss: 0.056\n",
      "[6, 800] loss: 0.054\n",
      "[6, 900] loss: 0.052\n",
      "[7, 100] loss: 0.051\n",
      "[7, 200] loss: 0.048\n",
      "[7, 300] loss: 0.052\n",
      "[7, 400] loss: 0.047\n",
      "[7, 500] loss: 0.047\n",
      "[7, 600] loss: 0.050\n",
      "[7, 700] loss: 0.052\n",
      "[7, 800] loss: 0.051\n",
      "[7, 900] loss: 0.057\n",
      "[8, 100] loss: 0.047\n",
      "[8, 200] loss: 0.045\n",
      "[8, 300] loss: 0.044\n",
      "[8, 400] loss: 0.047\n",
      "[8, 500] loss: 0.053\n",
      "[8, 600] loss: 0.046\n",
      "[8, 700] loss: 0.048\n",
      "[8, 800] loss: 0.051\n",
      "[8, 900] loss: 0.045\n",
      "[9, 100] loss: 0.045\n",
      "[9, 200] loss: 0.053\n",
      "[9, 300] loss: 0.048\n",
      "[9, 400] loss: 0.045\n",
      "[9, 500] loss: 0.041\n",
      "[9, 600] loss: 0.040\n",
      "[9, 700] loss: 0.046\n",
      "[9, 800] loss: 0.049\n",
      "[9, 900] loss: 0.035\n",
      "[10, 100] loss: 0.043\n",
      "[10, 200] loss: 0.039\n",
      "[10, 300] loss: 0.043\n",
      "[10, 400] loss: 0.041\n",
      "[10, 500] loss: 0.045\n",
      "[10, 600] loss: 0.043\n",
      "[10, 700] loss: 0.046\n",
      "[10, 800] loss: 0.036\n",
      "[10, 900] loss: 0.044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 98.27%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the test set: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw loss function for MNIST pytorch model \n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
